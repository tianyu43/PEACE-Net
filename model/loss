import torch
import torch.nn as nn
import torch.nn.functional as F



def SCL_loss(features, labels, temperature=0.07, alpha=0.1, p_th=0.5):
    """
    features: (B, d), 已归一化
    labels: (B,)
    temperature: 用于InfoNCE
    alpha: similarity loss 权重
    """
    device = features.device
    B = features.shape[0]
    labels =  torch.argmax(labels, dim=1)
    # Step 1: 相似度矩阵（归一化后点积 = 余弦相似度）
    sim_matrix = torch.matmul(features, features.T)  # shape (B, B)
    # Step 2: mask
    labels = labels.contiguous().view(-1, 1)
    mask = torch.eq(labels, labels.T).float().to(device)  # (B, B)
    logits = torch.div(sim_matrix, temperature)
    # Step 3: logits 去除自身
    logits_mask = torch.ones_like(mask) - torch.eye(B, device=device)
    logits = logits - torch.max(logits, dim=1, keepdim=True)[0].detach()  # 数值稳定
    exp_logits = torch.exp(logits) * logits_mask
    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))
    # Step 4: InfoNCE loss
    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)
    info_nce_loss = -mean_log_prob_pos.mean()
    # Step 5: Similarity Loss
    sim_pos = (1 - sim_matrix) * mask        
    sim_neg = (1 + sim_matrix) * (1 - mask)  
    sim_loss = (sim_pos.sum() + sim_neg.sum()) / (B * (B - 1))
    # Step 6: 总损失
    loss = alpha * info_nce_loss + sim_loss
    return loss, info_nce_loss.item(), sim_loss.item()


def nce_loss(features, labels, temperature=0.07, p_th=0.5):
    """
    features: (B, d), 已归一化
    labels: (B,)
    temperature: 用于InfoNCE
    """
    device = features.device
    B = features.shape[0]
    labels =  torch.argmax(labels, dim=1)
    # Step 1: 相似度矩阵（归一化后点积 = 余弦相似度）
    sim_matrix = torch.matmul(features, features.T)  # shape (B, B)
    # Step 2: mask
    labels = labels.contiguous().view(-1, 1)
    mask = torch.eq(labels, labels.T).float().to(device)  # (B, B)
    logits = torch.div(sim_matrix, temperature)
    # Step 3: logits 去除自身
    logits_mask = torch.ones_like(mask) - torch.eye(B, device=device)
    logits = logits - torch.max(logits, dim=1, keepdim=True)[0].detach()  # 数值稳定
    exp_logits = torch.exp(logits) * logits_mask
    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))
    # Step 4: InfoNCE loss
    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)
    info_nce_loss = -mean_log_prob_pos.mean()
    return info_nce_loss


def sim_loss(features, labels, margin=0.3):
    """
    features: (B, d), 已归一化
    labels: (B,)
    margin: float, margin for inter-class separation
    """
    device = features.device
    B = features.shape[0]
    labels =  torch.argmax(labels, dim=1)
    # 相似度矩阵（归一化后点积 = 余弦相似度）
    sim_matrix = torch.matmul(features, features.T)  # shape (B, B)
    # mask
    labels = labels.contiguous().view(-1, 1)
    mask = torch.eq(labels, labels.T).float().to(device)  # (B, B)
    # Similarity Loss
    sim_pos = (1 - sim_matrix) * mask        
    sim_neg = (1 + sim_matrix) * (1 - mask)  
    sim_loss = (sim_pos.sum() + sim_neg.sum()) / (B * (B - 1))
    return sim_loss



def sim_loss_with_margin(features, labels, margin=0.3):
    """
    Contrastive similarity loss with margin.
    
    Args:
        features: torch.Tensor of shape (B, d), L2-normalized feature embeddings
        labels: torch.Tensor of shape (B, C) or (B,), class labels or one-hot
        margin: float, margin for inter-class separation
    
    Returns:
        sim_loss: scalar, contrastive loss value
    """
    device = features.device
    B = features.shape[0]
    # If labels are one-hot, convert to class index
    if labels.ndim == 2:
        labels = torch.argmax(labels, dim=1)
    # Cosine similarity matrix (features should be normalized)
    sim_matrix = torch.matmul(features, features.T)  # (B, B), cosine similarities
    # Label mask
    labels = labels.contiguous().view(-1, 1)  # (B, 1)
    mask = torch.eq(labels, labels.T).float().to(device)  # (B, B), 1 for same class, 0 for different
    # Positive pairs: encourage similarity → loss = 1 - cos
    sim_pos = (1 - sim_matrix) * mask
    # Negative pairs: enforce margin → loss = max(0, cos - m)
    sim_neg = F.relu(sim_matrix - margin) * (1 - mask)
    # Exclude diagonal elements (i == j), as they are self-similarity (always 1)
    diag_mask = 1 - torch.eye(B, device=device)
    sim_pos = sim_pos * diag_mask
    sim_neg = sim_neg * diag_mask
    # Normalize by number of valid pairs (excluding diagonal)
    valid_pairs = B * (B - 1)
    loss = (sim_pos.sum() + sim_neg.sum()) / valid_pairs
    return loss



class SupConLoss(nn.Module):
    """Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.
    It also supports the unsupervised contrastive loss in SimCLR"""
    def __init__(self, temperature=0.07, contrast_mode='all',
                 base_temperature=0.07):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, features, labels=None, mask=None):
        """Compute loss for model. If both `labels` and `mask` are None,
        it degenerates to SimCLR unsupervised loss:
        https://arxiv.org/pdf/2002.05709.pdf

        Args:
            features: hidden vector of shape [bsz, n_views, ...].
            labels: ground truth of shape [bsz].
            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j
                has the same class as sample i. Can be asymmetric.
        Returns:
            A loss scalar.
        """
        device = (torch.device('cuda')
                  if features.is_cuda
                  else torch.device('cpu'))

        if len(features.shape) < 3:
            raise ValueError('`features` needs to be [bsz, n_views, ...],'
                             'at least 3 dimensions are required')
        if len(features.shape) > 3:
            features = features.view(features.shape[0], features.shape[1], -1)

        batch_size = features.shape[0]
        if labels is not None and mask is not None:
            raise ValueError('Cannot define both `labels` and `mask`')
        elif labels is None and mask is None:
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)
        elif labels is not None:
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)

        contrast_count = features.shape[1]
        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
        if self.contrast_mode == 'one':
            anchor_feature = features[:, 0]
            anchor_count = 1
        elif self.contrast_mode == 'all':
            anchor_feature = contrast_feature
            anchor_count = contrast_count
        else:
            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))

        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(anchor_feature, contrast_feature.T),
            self.temperature)
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # tile mask
        mask = mask.repeat(anchor_count, contrast_count)
        # mask-out self-contrast cases
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask
        '''
        with torch.no_grad():
            pos_pairs = mask.sum().item()  # 正样本对总数（矩阵中为1的项）
            total_pairs = logits_mask.sum().item()  # 总体可比较的 pair 数量（去掉对角线）
            neg_pairs = total_pairs - pos_pairs  # 负样本对数量

            print(f"[SupConLoss] 正样本对: {int(pos_pairs)}, 负样本对: {int(neg_pairs)}, 正负比: {pos_pairs / (neg_pairs + 1e-8):.4f}")
        '''

        # compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        # modified to handle edge cases when there is no positive pair
        # for an anchor point. 
        # Edge case e.g.:- 
        # features of shape: [4,1,...]
        # labels:            [0,1,1,2]
        # loss before mean:  [nan, ..., ..., nan] 
        mask_pos_pairs = mask.sum(1)
        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs
        
        
        
        # loss
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos
        loss = loss.view(anchor_count, batch_size).mean()

        return loss

